{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "551d5ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ipdb\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import mplcursors\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(style='darkgrid', context='notebook', rc={'figure.figsize':(14,10)}, font_scale=2)\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('chained_assignment',None)\n",
    "\n",
    "# Set random seeds for reproducibility on a specific machine\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "np.random.RandomState(1)\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    set_seed\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da5f7df",
   "metadata": {},
   "source": [
    "Control has the predictions of the best performing model at the task.\n",
    "\n",
    "Compare and contrast that with the model's performance I guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d8d282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv('../../data/cong_data.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e54f8c",
   "metadata": {},
   "source": [
    "What is the specificity of the training distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02c7258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.49, 0.53)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.mean(all_df[all_df['Split']=='train']['Specificity']), 2), np.round(np.std(all_df[all_df['Split']=='train']['Specificity']), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcbabe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "control = pd.read_csv('qual/predictions-control.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20be40d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('qual/predictions-dev-neg48.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cdd2c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['control'] = control['pred']\n",
    "df['pred'] = df['pred'].astype(int)\n",
    "df['control'] = df['control'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5193d1b6",
   "metadata": {},
   "source": [
    "# Where does prediction change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a9dd4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>group</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>affect</th>\n",
       "      <th>spec</th>\n",
       "      <th>pred</th>\n",
       "      <th>control</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>With the 2020 Census fast approaching, I joined @SenKamalaHarris and @senatorcarper in introducing legislation to ensure the approximately 10 million Americans who identify as LGBTQ are properly counted and represented in Census data collection efforts.</td>\n",
       "      <td>1</td>\n",
       "      <td>4.325275</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rest In Peace, @senjohnmccain. Thank you for serving our country with dignity and honor, as a Navy Pilot, a POW, elected official and a presidential candidate. You were a patriot and a true American hero. My deepest condolences and prayers to his family and loved ones.</td>\n",
       "      <td>0</td>\n",
       "      <td>4.142287</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1\\/3 Well said, @senatorlankford. We are $21 trillion in debt. Since 1974, we have passed all of our spending bills only 4 times! And, in the past 20 years, we have only passed a budget 11 times. We need to fix our broken budget and approps process.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.420717</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Today the House approved @sendougjones' &amp; my legislation to require the review, declassification &amp; release of government records related to unsolved criminal civil rights cases. --&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>4.088381</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Selma, AL has a special place in my heart. I was born here, and it’s also where my friend &amp; civil rights hero @repjohnlewis &amp; other activists crossed the Edmund Pettus Bridge while facing brutal opposition to their peaceful pursuit for civil rights.</td>\n",
       "      <td>1</td>\n",
       "      <td>4.396554</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Thank you, .@senschumer for thinking of including the Northern Marianas in your amendment to the disaster recovery appropriation bill. Our sincere gratitude for your help.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.851780</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Senator @chriscoons just asked Attorney General Barr:  \"What if a foreign adversary offers a presidential candidate dirt on a competitor in 2020. Do you agree with me the campaign should immediately contact the FBI?\"  Barr sat silent, unable to give a prompt response.</td>\n",
       "      <td>1</td>\n",
       "      <td>4.053232</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Thank you to @senatortester for introducing our bill, the Veteran HOUSE Act, in the Senate. Together, we can ensure veterans with OTH discharges, who are more likely to experience homelessness, can access the wraparound services they need.</td>\n",
       "      <td>1</td>\n",
       "      <td>4.195439</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Incredibly honored to host @speakerpelosi in Colorado today, along with CO Insurance Commissioner @mike_conway14 for a robust policy discussion on efforts being done at the state level and in the House of Representatives to #ProtectOurCare</td>\n",
       "      <td>1</td>\n",
       "      <td>4.444076</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>As @speakerpelosi says, the times have found each and every one of us to Defend our Democracy For The People.  Worth reading every line.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.217054</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>.@jim_jordan details out in a succinct &amp; clear way how this entire hoax being led by Adam Schiff is nothing like the leaks &amp; narrative that was rehearsed by House Democrats over the last few months. It’s time for this ridiculous charade to stop. #EndTheWitchHunt #TotalSchiffShow</td>\n",
       "      <td>1</td>\n",
       "      <td>4.187413</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Wishing my friend @repjohnlewis, a man who has more fight in him than anyone I know in Congress, continued strength and courage in his battle. A true American hero.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.584385</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Congrats @repcharliecrist on the big win to protect seniors from annoying and fraudulent #robocalls #ForThePeople! Honored to work with you on your great bill!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>3.553932</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Increasing mobile testing units in vulnerable communities would help us to track and mitigate the spread of COVID-19. @repsylviagarcia &amp; I are fighting for mobile testing units in communities that need it most. Read our statement here:</td>\n",
       "      <td>1</td>\n",
       "      <td>3.811517</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>.@repjohnlewis was a leader who challenged the status quo and helped change our nation for the better... He was a principled gentleman and a friend. He will be missed.   My full statement:</td>\n",
       "      <td>0</td>\n",
       "      <td>3.526774</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>\"I just want to say as a member of this body, as 1\\/435 of 1\\/2 of 1\\/3 of the federal government, I believe this country is great &amp; I'm proud of this country. Unapologetically proud to be an American.\"  Thank you to my fellow Texan, @repchiproy, for joining me in #DefendingAmerica:</td>\n",
       "      <td>1</td>\n",
       "      <td>3.023670</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>This is what oversight looks like. We are not going to sit down and shut up while Bill Barr politicizes the Department of Justice &amp; makes a mockery of the rule of law. Great job @repjayapal. Everyone should watch.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.814559</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>.@speakerpelosi’s HEROES Act mentions marijuana more times than jobs, showing that Democrats aren’t serious about providing relief to Americans. Republicans offered to extend unemployment benefits, but Pelosi refused. What’s it going to take to bring Democrats to the table?</td>\n",
       "      <td>0</td>\n",
       "      <td>3.709809</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Many are asking me about the behavior of certain members of Congress and how the House plans to address it. Regretfully, the Ethics Committee - on which I serve - isn’t yet operational as we await @gopleader McCarthy to name Republican members.</td>\n",
       "      <td>0</td>\n",
       "      <td>3.831910</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Republicans’ attacks on @ilhan are an absurd distraction.  Republicans may have a difference of opinion on policy with Rep. Omar. Opinions on policy are much different than the violence, hate and QAnon conspiracy theories Rep. Taylor Greene continues to believe and espouse.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.948154</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>I’m saddened by the passing of @repronwright.   He served Texas with admirable dedication and passion.  His loved ones and constituents are in my prayers.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.609238</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Count me in, @repmondaire!  If we don’t #ExpandTheCourt, the 6-3 radical right-wing SCOTUS majority is poised to:  ??Suppress the voices of Black and brown Americans at the ballot box ⏪ Rollback our reproductive rights ❌Block comprehensive, humane immigration reform</td>\n",
       "      <td>1</td>\n",
       "      <td>4.321592</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Joined @sentedcruz &amp; 31 GOP Senators in cosponsoring legislation to repeal #ObamaCare.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.556749</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>I know Leader McConnell is very proud of his #Louisville #Cardinals! Congrats to them on a great season. #FinalFour cc: @mcconnellpress</td>\n",
       "      <td>1</td>\n",
       "      <td>3.538630</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Watch T&amp;I Army Corps Prehearing dialogue with @repmullin - Chief's Reports over 9000 pages   #sctweets #WaterResource</td>\n",
       "      <td>1</td>\n",
       "      <td>3.674071</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Long day for staff too MT .@repronbarber: My military advisor Jeremy's reaction to 16hour #NDAA markup \"It's what we came to Congress to do\"</td>\n",
       "      <td>1</td>\n",
       "      <td>3.656615</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>.@senwhitehouse on why juries are key to our justice system:   It's why I introduced H.R. 1844:</td>\n",
       "      <td>1</td>\n",
       "      <td>2.997340</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Know how federal regs are costing you? Tune in around 7 to     to see me, @repdougcollins, and more talk #regrelief</td>\n",
       "      <td>1</td>\n",
       "      <td>3.122366</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>My full statement with @keithellison on today’s harmful #SCOTUS #VRA ruling. We need #RightToVote 4 all.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.028286</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Thanks @repsheaporter 4 cosponsoring the Expedited Hiring for VA Trained Psychiatrists Act &amp; working 2 provide greater access to our #Vets</td>\n",
       "      <td>1</td>\n",
       "      <td>3.725444</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Questions from @roslehtinen touch on costs of war... Kerry repeating \"no boots on ground\" #syria</td>\n",
       "      <td>1</td>\n",
       "      <td>3.176985</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Thx to all the people from #Illinois &amp; @senatordurbin for the Bday wishes at our constituent coffee this morning.</td>\n",
       "      <td>0</td>\n",
       "      <td>3.065225</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>BREAKING: Pleased to announce @roslehtinen as a co-sponsor of my #CIR bill. Together we are moving #SouthFlorida forward</td>\n",
       "      <td>0</td>\n",
       "      <td>3.302114</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>In honor of @waxmanclimate announcing his retirement today...   #tbt #throwbackthursday</td>\n",
       "      <td>0</td>\n",
       "      <td>3.019805</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>.@sengillibrand thank you for your leadership. #passMJIA</td>\n",
       "      <td>0</td>\n",
       "      <td>2.300448</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Hearing as Ranking Member on #bioterrorism happening now w\\/ my friend and mentor .@billpascrell WATCH</td>\n",
       "      <td>1</td>\n",
       "      <td>3.387834</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Paula is one of the #FacesoftheUnemployed &amp; had always worked since she was 15 years old. #RenewUI @speakerboehner.</td>\n",
       "      <td>0</td>\n",
       "      <td>3.344323</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Thank you to @reppittenger for his advocacy on behalf of the family and the abused.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.012389</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Pleased @senatorburr joined me to intro bill ending bonuses at Veterans Health Adm. amid troubling waitlist scandal</td>\n",
       "      <td>1</td>\n",
       "      <td>3.972801</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>.@repcartwright on why we must come together to prepare for #ExtremeWeather   #SafeClimateCaucus</td>\n",
       "      <td>1</td>\n",
       "      <td>2.687845</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Senator McConnell Lauds @senalexander's NLRB Reform Legislation</td>\n",
       "      <td>1</td>\n",
       "      <td>3.622458</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>My stmt w\\/ @grahamblog today on #Turkey, #Kobani &amp; and the fight against #ISIS:</td>\n",
       "      <td>1</td>\n",
       "      <td>3.120435</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>MT @repmcnerney Next year’s max Pell Grant will cover smallest share of college costs in program's history. Time to #jumpstart middle class.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.913582</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>This is your birthday tweet. It’s short and sweet. Hey! @speakerboehner</td>\n",
       "      <td>1</td>\n",
       "      <td>2.410680</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Congratulations to my friend, Congressman @gkbutterfield, on his election as Chair of the @officialCBC for the 114th Congress!</td>\n",
       "      <td>1</td>\n",
       "      <td>3.576348</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Amir Hekmati fought for our freedom as a Marine and he must be returned. Thanks @txrandy14 for cosponsoring #HRes233. #FreeAmirNow</td>\n",
       "      <td>0</td>\n",
       "      <td>3.744590</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Thanks to @RepRyanCostello, @repmimiwalters, @rephardy, @repcurbelo, @hurdonthehill (1\\/2)</td>\n",
       "      <td>1</td>\n",
       "      <td>2.356794</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Watch me ask for Judiciary hearings on the many gun safety proposals out there, like @repmikequigley's TRACE Act</td>\n",
       "      <td>1</td>\n",
       "      <td>3.300089</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Thank you for many years of #leadership @speakerboehner. Will think abt you anytime I sing the infamous ‘Boehner birthday song’ to my staff!</td>\n",
       "      <td>1</td>\n",
       "      <td>3.439807</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>Happy Birthday to our new Speaker of the House! @speakerryan</td>\n",
       "      <td>1</td>\n",
       "      <td>3.046144</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>.@senfranken is a leader on so many issues. Glad to welcome him to twitter, very much worth following.</td>\n",
       "      <td>1</td>\n",
       "      <td>2.588714</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Happy Birthday, @repjohnlewis! Hope you had a great day.</td>\n",
       "      <td>0</td>\n",
       "      <td>2.329201</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>#EWApprops bill, led by Alexander &amp; @senfeinstein, unanimously passes full committee &amp; ready for floor</td>\n",
       "      <td>0</td>\n",
       "      <td>3.638373</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Happy Birthday @corybooker! I got you a new bill:   #IIOA</td>\n",
       "      <td>0</td>\n",
       "      <td>2.885369</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Thank you to @repmarktakai for cosponsoring our HUBZone Redesignation Act. Very important legislation.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.107474</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Students with education loans should have access to financial counseling. @repguthrie’s bill fixes this problem.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.287926</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Saddened to hear of death of my HASC colleague @repmarktakai due to cancer. He was a patriot serving in Army Guard &amp; devoted Rep. of Hawaii.</td>\n",
       "      <td>0</td>\n",
       "      <td>3.952471</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Fun @timkaine fact: he is very very good at baseball trivia. Which I hear was very high on Hillary's qualifications list.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.148445</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>.@senatorboxer on need to pass Lake Tahoe Restoration Act: We are here to celebrate the progress but we have more work to do. #KeepTahoeBlue</td>\n",
       "      <td>1</td>\n",
       "      <td>3.400074</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Thank you, @reprichhudson. This bill will help save the lives of patients in need. Proud to have led this effort with you.</td>\n",
       "      <td>0</td>\n",
       "      <td>3.212949</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>Trump selected @reptomprice for HHS Secretary. Price has undeniable history of cutting access to healthcare to millions, especially women</td>\n",
       "      <td>0</td>\n",
       "      <td>3.928363</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>\"We don't want to go back. We want to go forward.\" Here's why @repjohnlewis is known as the conscience of Congress.</td>\n",
       "      <td>1</td>\n",
       "      <td>2.785141</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>Hatch asked Gorsuch about @senschumer's past praise for judges who put the law over \"sympathy.\" @GorsuchFacts GorsuchFacts #tcot #utpol</td>\n",
       "      <td>0</td>\n",
       "      <td>3.648393</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>.@replloyddoggett rightly points out that Ways\\/Means must get #Trumptaxes 2 do proper oversight &amp; b\\/c of coming consideration of tax reform.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.290197</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>Waiting in line at the Capitol to film short message to my friend @stevescalise &amp; sporting Clemson tie with LSU hat #GeauxTigers</td>\n",
       "      <td>1</td>\n",
       "      <td>3.880189</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>.@ronwyden &amp; I are joining Oregonians to stand up &amp; fight back against diabolical GOP #HealthCareBill. RT to stand with us! #ProtectOurCare</td>\n",
       "      <td>1</td>\n",
       "      <td>3.345546</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Honored to receive the 2017 Lewis-Houghton Award from @repjohnlewis &amp; @FaithNPolitics for conscience, courage, and compassion</td>\n",
       "      <td>0</td>\n",
       "      <td>3.675850</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>My friend @reprichardneal and I wrote to Secretary Price w\\/ serious concerns over bogus HHS analysis on #Trumpcare.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.545793</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>Shouts of Joy &amp; thanks be to God! @stevescalise is discharged from the hospital.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.147912</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>Happy birthday, @pattymurray!</td>\n",
       "      <td>0</td>\n",
       "      <td>2.143305</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>The House just passed @repandybarr’s bill to add new sanctions preventing North Korea from being able to finance their weapons program.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.886021</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>Americans deserve a #BetterDeal than this #GOPTaxScam. Democrats will provide that with higher wages and better opportunity. Proud to have @repcicilline join me on the floor tonight.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.708146</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Congratulations on an outstanding tenure in the Senate, @senorrinhatch. The state of Utah and our country are better off because of your distinguished service, and I look forward to continuing to work with you over the next year.</td>\n",
       "      <td>1</td>\n",
       "      <td>4.220261</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>Our veterans deserve far better treatment than what they currently receive. Proud to join @repwesterman in introducing legislation to improve compensation for Vietnam-era veterans who need care. &gt;&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>3.820121</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>A clip from yesterday's hearing where Sen. @martinheinrich questioned me on the NMI US Workforce Act.</td>\n",
       "      <td>1</td>\n",
       "      <td>3.605187</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Happy birthday to my friend and fellow Granite Stater in the Senate, @senatorhassan!</td>\n",
       "      <td>1</td>\n",
       "      <td>3.353239</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>Extremely sad to hear of the loss of my colleague &amp; dean of the NY delegation @louiseslaughter. She recently brought together the NY delegation in her office. Louise was a staunch advocate for her beliefs and her legacy will forever grace the halls of Congress.</td>\n",
       "      <td>0</td>\n",
       "      <td>4.460001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                             tweet  \\\n",
       "3                                    With the 2020 Census fast approaching, I joined @SenKamalaHarris and @senatorcarper in introducing legislation to ensure the approximately 10 million Americans who identify as LGBTQ are properly counted and represented in Census data collection efforts.   \n",
       "4                    Rest In Peace, @senjohnmccain. Thank you for serving our country with dignity and honor, as a Navy Pilot, a POW, elected official and a presidential candidate. You were a patriot and a true American hero. My deepest condolences and prayers to his family and loved ones.   \n",
       "8                                        1\\/3 Well said, @senatorlankford. We are $21 trillion in debt. Since 1974, we have passed all of our spending bills only 4 times! And, in the past 20 years, we have only passed a budget 11 times. We need to fix our broken budget and approps process.   \n",
       "9                                                                                                          Today the House approved @sendougjones' & my legislation to require the review, declassification & release of government records related to unsolved criminal civil rights cases. -->     \n",
       "20                                     Selma, AL has a special place in my heart. I was born here, and it’s also where my friend & civil rights hero @repjohnlewis & other activists crossed the Edmund Pettus Bridge while facing brutal opposition to their peaceful pursuit for civil rights.     \n",
       "26                                                                                                                     Thank you, .@senschumer for thinking of including the Northern Marianas in your amendment to the disaster recovery appropriation bill. Our sincere gratitude for your help.   \n",
       "28                    Senator @chriscoons just asked Attorney General Barr:  \"What if a foreign adversary offers a presidential candidate dirt on a competitor in 2020. Do you agree with me the campaign should immediately contact the FBI?\"  Barr sat silent, unable to give a prompt response.   \n",
       "33                                               Thank you to @senatortester for introducing our bill, the Veteran HOUSE Act, in the Senate. Together, we can ensure veterans with OTH discharges, who are more likely to experience homelessness, can access the wraparound services they need.     \n",
       "38                                               Incredibly honored to host @speakerpelosi in Colorado today, along with CO Insurance Commissioner @mike_conway14 for a robust policy discussion on efforts being done at the state level and in the House of Representatives to #ProtectOurCare     \n",
       "41                                                                                                                                                      As @speakerpelosi says, the times have found each and every one of us to Defend our Democracy For The People.  Worth reading every line.     \n",
       "43         .@jim_jordan details out in a succinct & clear way how this entire hoax being led by Adam Schiff is nothing like the leaks & narrative that was rehearsed by House Democrats over the last few months. It’s time for this ridiculous charade to stop. #EndTheWitchHunt #TotalSchiffShow   \n",
       "48                                                                                                                            Wishing my friend @repjohnlewis, a man who has more fight in him than anyone I know in Congress, continued strength and courage in his battle. A true American hero.   \n",
       "49                                                                                                                               Congrats @repcharliecrist on the big win to protect seniors from annoying and fraudulent #robocalls #ForThePeople! Honored to work with you on your great bill!!!   \n",
       "61                                                   Increasing mobile testing units in vulnerable communities would help us to track and mitigate the spread of COVID-19. @repsylviagarcia & I are fighting for mobile testing units in communities that need it most. Read our statement here:     \n",
       "67                                                                                                .@repjohnlewis was a leader who challenged the status quo and helped change our nation for the better... He was a principled gentleman and a friend. He will be missed.   My full statement:       \n",
       "68   \"I just want to say as a member of this body, as 1\\/435 of 1\\/2 of 1\\/3 of the federal government, I believe this country is great & I'm proud of this country. Unapologetically proud to be an American.\"  Thank you to my fellow Texan, @repchiproy, for joining me in #DefendingAmerica:     \n",
       "70                                                                           This is what oversight looks like. We are not going to sit down and shut up while Bill Barr politicizes the Department of Justice & makes a mockery of the rule of law. Great job @repjayapal. Everyone should watch.   \n",
       "73            .@speakerpelosi’s HEROES Act mentions marijuana more times than jobs, showing that Democrats aren’t serious about providing relief to Americans. Republicans offered to extend unemployment benefits, but Pelosi refused. What’s it going to take to bring Democrats to the table?     \n",
       "79                                            Many are asking me about the behavior of certain members of Congress and how the House plans to address it. Regretfully, the Ethics Committee - on which I serve - isn’t yet operational as we await @gopleader McCarthy to name Republican members.   \n",
       "80              Republicans’ attacks on @ilhan are an absurd distraction.  Republicans may have a difference of opinion on policy with Rep. Omar. Opinions on policy are much different than the violence, hate and QAnon conspiracy theories Rep. Taylor Greene continues to believe and espouse.   \n",
       "82                                                                                                                                      I’m saddened by the passing of @repronwright.   He served Texas with admirable dedication and passion.  His loved ones and constituents are in my prayers.   \n",
       "92                      Count me in, @repmondaire!  If we don’t #ExpandTheCourt, the 6-3 radical right-wing SCOTUS majority is poised to:  ??Suppress the voices of Black and brown Americans at the ballot box ⏪ Rollback our reproductive rights ❌Block comprehensive, humane immigration reform   \n",
       "94                                                                                                                                                                                                          Joined @sentedcruz & 31 GOP Senators in cosponsoring legislation to repeal #ObamaCare.   \n",
       "98                                                                                                                                                         I know Leader McConnell is very proud of his #Louisville #Cardinals! Congrats to them on a great season. #FinalFour cc: @mcconnellpress   \n",
       "104                                                                                                                                                                          Watch T&I Army Corps Prehearing dialogue with @repmullin - Chief's Reports over 9000 pages   #sctweets #WaterResource   \n",
       "106                                                                                                                                                   Long day for staff too MT .@repronbarber: My military advisor Jeremy's reaction to 16hour #NDAA markup \"It's what we came to Congress to do\"   \n",
       "107                                                                                                                                                                                              .@senwhitehouse on why juries are key to our justice system:   It's why I introduced H.R. 1844:     \n",
       "109                                                                                                                                                                            Know how federal regs are costing you? Tune in around 7 to     to see me, @repdougcollins, and more talk #regrelief   \n",
       "110                                                                                                                                                                                     My full statement with @keithellison on today’s harmful #SCOTUS #VRA ruling. We need #RightToVote 4 all.     \n",
       "111                                                                                                                                                     Thanks @repsheaporter 4 cosponsoring the Expedited Hiring for VA Trained Psychiatrists Act & working 2 provide greater access to our #Vets   \n",
       "117                                                                                                                                                                                               Questions from @roslehtinen touch on costs of war... Kerry repeating \"no boots on ground\" #syria   \n",
       "119                                                                                                                                                                            Thx to all the people from #Illinois & @senatordurbin for the Bday wishes at our constituent coffee this morning.     \n",
       "127                                                                                                                                                                       BREAKING: Pleased to announce @roslehtinen as a co-sponsor of my #CIR bill. Together we are moving #SouthFlorida forward   \n",
       "133                                                                                                                                                                                                        In honor of @waxmanclimate announcing his retirement today...   #tbt #throwbackthursday   \n",
       "134                                                                                                                                                                                                                                       .@sengillibrand thank you for your leadership. #passMJIA   \n",
       "135                                                                                                                                                                                       Hearing as Ranking Member on #bioterrorism happening now w\\/ my friend and mentor .@billpascrell WATCH     \n",
       "140                                                                                                                                                                          Paula is one of the #FacesoftheUnemployed & had always worked since she was 15 years old. #RenewUI @speakerboehner.     \n",
       "142                                                                                                                                                                                                            Thank you to @reppittenger for his advocacy on behalf of the family and the abused.   \n",
       "143                                                                                                                                                                          Pleased @senatorburr joined me to intro bill ending bonuses at Veterans Health Adm. amid troubling waitlist scandal     \n",
       "144                                                                                                                                                                                             .@repcartwright on why we must come together to prepare for #ExtremeWeather   #SafeClimateCaucus     \n",
       "154                                                                                                                                                                                                                              Senator McConnell Lauds @senalexander's NLRB Reform Legislation     \n",
       "157                                                                                                                                                                                                             My stmt w\\/ @grahamblog today on #Turkey, #Kobani & and the fight against #ISIS:     \n",
       "158                                                                                                                                                   MT @repmcnerney Next year’s max Pell Grant will cover smallest share of college costs in program's history. Time to #jumpstart middle class.   \n",
       "160                                                                                                                                                                                                                        This is your birthday tweet. It’s short and sweet. Hey! @speakerboehner   \n",
       "162                                                                                                                                                                 Congratulations to my friend, Congressman @gkbutterfield, on his election as Chair of the @officialCBC for the 114th Congress!   \n",
       "176                                                                                                                                                             Amir Hekmati fought for our freedom as a Marine and he must be returned. Thanks @txrandy14 for cosponsoring #HRes233. #FreeAmirNow   \n",
       "180                                                                                                                                                                                                     Thanks to @RepRyanCostello, @repmimiwalters, @rephardy, @repcurbelo, @hurdonthehill (1\\/2)   \n",
       "194                                                                                                                                                                             Watch me ask for Judiciary hearings on the many gun safety proposals out there, like @repmikequigley's TRACE Act     \n",
       "196                                                                                                                                                   Thank you for many years of #leadership @speakerboehner. Will think abt you anytime I sing the infamous ‘Boehner birthday song’ to my staff!   \n",
       "205                                                                                                                                                                                                                                 Happy Birthday to our new Speaker of the House! @speakerryan     \n",
       "207                                                                                                                                                                                       .@senfranken is a leader on so many issues. Glad to welcome him to twitter, very much worth following.     \n",
       "210                                                                                                                                                                                                                                       Happy Birthday, @repjohnlewis! Hope you had a great day.   \n",
       "212                                                                                                                                                                                         #EWApprops bill, led by Alexander & @senfeinstein, unanimously passes full committee & ready for floor   \n",
       "216                                                                                                                                                                                                                                      Happy Birthday @corybooker! I got you a new bill:   #IIOA   \n",
       "219                                                                                                                                                                                         Thank you to @repmarktakai for cosponsoring our HUBZone Redesignation Act. Very important legislation.   \n",
       "229                                                                                                                                                                             Students with education loans should have access to financial counseling. @repguthrie’s bill fixes this problem.     \n",
       "231                                                                                                                                                   Saddened to hear of death of my HASC colleague @repmarktakai due to cancer. He was a patriot serving in Army Guard & devoted Rep. of Hawaii.   \n",
       "232                                                                                                                                                                      Fun @timkaine fact: he is very very good at baseball trivia. Which I hear was very high on Hillary's qualifications list.   \n",
       "236                                                                                                                                                   .@senatorboxer on need to pass Lake Tahoe Restoration Act: We are here to celebrate the progress but we have more work to do. #KeepTahoeBlue   \n",
       "239                                                                                                                                                                   Thank you, @reprichhudson. This bill will help save the lives of patients in need. Proud to have led this effort with you.     \n",
       "241                                                                                                                                                      Trump selected @reptomprice for HHS Secretary. Price has undeniable history of cutting access to healthcare to millions, especially women   \n",
       "247                                                                                                                                                                          \"We don't want to go back. We want to go forward.\" Here's why @repjohnlewis is known as the conscience of Congress.     \n",
       "256                                                                                                                                                      Hatch asked Gorsuch about @senschumer's past praise for judges who put the law over \"sympathy.\" @GorsuchFacts GorsuchFacts #tcot #utpol     \n",
       "257                                                                                                                                                 .@replloyddoggett rightly points out that Ways\\/Means must get #Trumptaxes 2 do proper oversight & b\\/c of coming consideration of tax reform.   \n",
       "262                                                                                                                                                             Waiting in line at the Capitol to film short message to my friend @stevescalise & sporting Clemson tie with LSU hat #GeauxTigers     \n",
       "263                                                                                                                                                  .@ronwyden & I are joining Oregonians to stand up & fight back against diabolical GOP #HealthCareBill. RT to stand with us! #ProtectOurCare     \n",
       "265                                                                                                                                                                Honored to receive the 2017 Lewis-Houghton Award from @repjohnlewis & @FaithNPolitics for conscience, courage, and compassion     \n",
       "266                                                                                                                                                                         My friend @reprichardneal and I wrote to Secretary Price w\\/ serious concerns over bogus HHS analysis on #Trumpcare.     \n",
       "267                                                                                                                                                                                                             Shouts of Joy & thanks be to God! @stevescalise is discharged from the hospital.     \n",
       "274                                                                                                                                                                                                                                                                  Happy birthday, @pattymurray!   \n",
       "276                                                                                                                                                      The House just passed @repandybarr’s bill to add new sanctions preventing North Korea from being able to finance their weapons program.     \n",
       "282                                                                                                         Americans deserve a #BetterDeal than this #GOPTaxScam. Democrats will provide that with higher wages and better opportunity. Proud to have @repcicilline join me on the floor tonight.   \n",
       "284                                                          Congratulations on an outstanding tenure in the Senate, @senorrinhatch. The state of Utah and our country are better off because of your distinguished service, and I look forward to continuing to work with you over the next year.   \n",
       "288                                                                                        Our veterans deserve far better treatment than what they currently receive. Proud to join @repwesterman in introducing legislation to improve compensation for Vietnam-era veterans who need care. >>     \n",
       "294                                                                                                                                                                                       A clip from yesterday's hearing where Sen. @martinheinrich questioned me on the NMI US Workforce Act.      \n",
       "297                                                                                                                                                                                                         Happy birthday to my friend and fellow Granite Stater in the Senate, @senatorhassan!     \n",
       "301                        Extremely sad to hear of the loss of my colleague & dean of the NY delegation @louiseslaughter. She recently brought together the NY delegation in her office. Louise was a staunch advocate for her beliefs and her legacy will forever grace the halls of Congress.     \n",
       "\n",
       "     group  Specificity  affect  spec  pred  control  \n",
       "3        1     4.325275       1     1     0        1  \n",
       "4        0     4.142287       1     1     0        1  \n",
       "8        1     3.420717       1   100     0        1  \n",
       "9        0     4.088381       1     1     0        1  \n",
       "20       1     4.396554       1     1     0        1  \n",
       "26       1     3.851780       1   100     0        1  \n",
       "28       1     4.053232       0     1     0        1  \n",
       "33       1     4.195439       1     1     0        1  \n",
       "38       1     4.444076       1     1     0        1  \n",
       "41       1     3.217054       1   100     0        1  \n",
       "43       1     4.187413       1     1     0        1  \n",
       "48       1     3.584385       1   100     0        1  \n",
       "49       1     3.553932       1   100     0        1  \n",
       "61       1     3.811517       1   100     0        1  \n",
       "67       0     3.526774       1   100     0        1  \n",
       "68       1     3.023670       1   100     0        1  \n",
       "70       1     3.814559       1   100     0        1  \n",
       "73       0     3.709809       0   100     0        1  \n",
       "79       0     3.831910       0   100     0        1  \n",
       "80       1     3.948154       0   100     0        1  \n",
       "82       1     3.609238       1   100     0        1  \n",
       "92       1     4.321592       1     1     0        1  \n",
       "94       1     3.556749       1   100     0        1  \n",
       "98       1     3.538630       0   100     0        1  \n",
       "104      1     3.674071       0   100     0        1  \n",
       "106      1     3.656615       0   100     0        1  \n",
       "107      1     2.997340       1     0     0        1  \n",
       "109      1     3.122366       1   100     0        1  \n",
       "110      1     3.028286       1   100     0        1  \n",
       "111      1     3.725444       1   100     0        1  \n",
       "117      1     3.176985       0   100     0        1  \n",
       "119      0     3.065225       1   100     0        1  \n",
       "127      0     3.302114       1   100     0        1  \n",
       "133      0     3.019805       1   100     0        1  \n",
       "134      0     2.300448       1     0     0        1  \n",
       "135      1     3.387834       1   100     0        1  \n",
       "140      0     3.344323       0   100     0        1  \n",
       "142      1     3.012389       1   100     0        1  \n",
       "143      1     3.972801       1   100     0        1  \n",
       "144      1     2.687845       1     0     0        1  \n",
       "154      1     3.622458       1   100     0        1  \n",
       "157      1     3.120435       1   100     0        1  \n",
       "158      1     3.913582       0   100     0        1  \n",
       "160      1     2.410680       1     0     0        1  \n",
       "162      1     3.576348       1   100     0        1  \n",
       "176      0     3.744590       1   100     0        1  \n",
       "180      1     2.356794       0     0     0        1  \n",
       "194      1     3.300089       1   100     0        1  \n",
       "196      1     3.439807       1   100     0        1  \n",
       "205      1     3.046144       1   100     0        1  \n",
       "207      1     2.588714       1     0     0        1  \n",
       "210      0     2.329201       1     0     0        1  \n",
       "212      0     3.638373       1   100     0        1  \n",
       "216      0     2.885369       1     0     0        1  \n",
       "219      1     3.107474       1   100     0        1  \n",
       "229      1     3.287926       1   100     0        1  \n",
       "231      0     3.952471       1   100     0        1  \n",
       "232      1     3.148445       1   100     0        1  \n",
       "236      1     3.400074       1   100     0        1  \n",
       "239      0     3.212949       1   100     0        1  \n",
       "241      0     3.928363       0   100     0        1  \n",
       "247      1     2.785141       1     0     0        1  \n",
       "256      0     3.648393       0   100     0        1  \n",
       "257      1     3.290197       1   100     0        1  \n",
       "262      1     3.880189       1   100     0        1  \n",
       "263      1     3.345546       1   100     0        1  \n",
       "265      0     3.675850       1   100     0        1  \n",
       "266      1     3.545793       1   100     0        1  \n",
       "267      1     3.147912       1   100     0        1  \n",
       "274      0     2.143305       1     0     0        1  \n",
       "276      1     3.886021       0   100     0        1  \n",
       "282      1     3.708146       1   100     0        1  \n",
       "284      1     4.220261       1     1     0        1  \n",
       "288      0     3.820121       1   100     0        1  \n",
       "294      1     3.605187       0   100     0        1  \n",
       "297      1     3.353239       1   100     0        1  \n",
       "301      0     4.460001       1     1     0        1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['pred']==0) & (df['control']==1)].loc[:, ['tweet', 'group', 'Specificity', 'affect', 'spec', 'pred', 'control']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bc04bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 56, 100: 212, 0: 38})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df['spec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0fb5bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({100: 1971, 1: 579, 0: 483})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def specificity_calc(x):\n",
    "   '''\n",
    "   0 if specificity is less than 3, and 1 if specificity is greater than 4\n",
    "   '''\n",
    "\n",
    "   if (x-3.0) < 0.00001:\n",
    "      return 0\n",
    "   elif (x-4.0) > 0.00001:\n",
    "      return 1\n",
    "   else:\n",
    "      return 100\n",
    "\n",
    "all_df['affect'] = all_df.apply(lambda x: 1 if (x['Feeling']=='warm' or x['Behavior']=='app') else 0, axis=1)\n",
    "\n",
    "all_df['spec'] = all_df['Specificity'].apply(lambda x: specificity_calc(x))\n",
    "Counter(all_df['spec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d86ec16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({1: 1451, 0: 520}),\n",
       " Counter({1: 477, 0: 102}),\n",
       " Counter({1: 353, 0: 130}))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(all_df[all_df['spec']==100]['affect']), Counter(all_df[all_df['spec']==1]['affect']), Counter(all_df[all_df['spec']==0]['affect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d5678a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Intervention hook function\n",
    "def intervention(h_out, P, ws, alpha):\n",
    "    '''\n",
    "    Perform positive or negative intervention on all tokens\n",
    "    '''\n",
    "    # Take out the cls token\n",
    "    h_tokens = h_out[0][:,1:,:]\n",
    "\n",
    "    # AlterRep code starts here\n",
    "    signs = torch.sign(h_tokens@ws.T).long()\n",
    "\n",
    "    # h_r component\n",
    "    proj = (h_tokens@ws.T)\n",
    "    if alpha>=0:\n",
    "        proj = proj * signs\n",
    "    else:\n",
    "        proj = proj * (-signs)\n",
    "    h_r = proj@ws*np.abs(alpha)\n",
    "\n",
    "    # Get vector only in the direction of perpendicular to decision boundary\n",
    "    h_n = h_tokens@P\n",
    "\n",
    "    # Now pushing it either in positive or negative intervention direction\n",
    "    h_alter = h_n + h_r\n",
    "\n",
    "    # Return h_alter concatenated with the cls token\n",
    "    return (torch.cat((h_out[0][:,:1,:], h_alter), dim=1),)\n",
    "\n",
    "from inlp.debias import debias_by_specific_directions\n",
    "\n",
    "with open(\"../reps_affect/Ws.layer=11.seed=1.npy\", \"rb\") as f:\n",
    "    Ws = np.load(f)\n",
    "\n",
    "        \n",
    "# Reduce Ws to number of classifiers you want to set it to\n",
    "Ws = Ws[:32:]\n",
    "\n",
    "# Now derive P from Ws\n",
    "list_of_ws = [np.array([Ws[i, :]]) for i in range(Ws.shape[0])]\n",
    "P = debias_by_specific_directions(directions=list_of_ws, input_dim=Ws.shape[1])\n",
    "\n",
    "Ws_aff = torch.tensor(Ws/np.linalg.norm(Ws, keepdims = True, axis = 1)).to(torch.float32)\n",
    "P_aff = torch.tensor(P).to(torch.float32)\n",
    "\n",
    "# Insert newaxis for 1 classifier edge case\n",
    "if len(Ws_aff.shape) == 1:\n",
    "    Ws_aff = Ws_aff[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e81403f",
   "metadata": {},
   "source": [
    "## What does the intervention do?\n",
    "\n",
    "Here I want to apply the intervention on individual tokens, and see if it makes them more of a certain affect, or more general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04ed732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Happy <mask> @corybooker! I got you a new bill: #IIOA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5156b8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../saved_models/bertweet-base_proto_0_seed_1_epoch_3/ were not used when initializing RobertaForMaskedLM: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at ../saved_models/bertweet-base_proto_0_seed_1_epoch_3/ and are newly initialized: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/status/@@',\n",
       " 'ousem@@',\n",
       " 'sworn',\n",
       " 'LYR@@',\n",
       " 'Timo@@',\n",
       " 'giar@@',\n",
       " 'opport@@',\n",
       " 'Narr@@',\n",
       " '#tvtime',\n",
       " 'Eup@@']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(1)\n",
    "tokenizer = AutoTokenizer.from_pretrained('../saved_models/bertweet-base_proto_0_seed_1_epoch_3/', use_fast=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained('../saved_models/bertweet-base_proto_0_seed_1_epoch_3/')\n",
    "input_dict=tokenizer(text, return_tensors='pt')\n",
    "logits = torch.log_softmax(model(**input_dict)['logits'], dim=2)\n",
    "mask_token_index = torch.where(input_dict['input_ids'][0]==tokenizer.mask_token_id)[0].detach().cpu().tolist()[0]\n",
    "tokenizer.convert_ids_to_tokens(torch.topk(logits[0,mask_token_index,:], 10)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3d0ae2",
   "metadata": {},
   "source": [
    "## Vanilla model use here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8641c2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['birthday',\n",
       " 'Birthday',\n",
       " 'new',\n",
       " '#@@',\n",
       " 'New',\n",
       " 'anniversary',\n",
       " '4th',\n",
       " 'Anniversary',\n",
       " '21st',\n",
       " 'belated']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(1)\n",
    "tokenizer2 = AutoTokenizer.from_pretrained('vinai/bertweet-base', use_fast=True)\n",
    "model2 = AutoModelForMaskedLM.from_pretrained('vinai/bertweet-base')\n",
    "input_dict=tokenizer2(text, return_tensors='pt')\n",
    "logits = torch.log_softmax(model2(**input_dict)['logits'], dim=2)\n",
    "mask_token_index = torch.where(input_dict['input_ids'][0]==tokenizer2.mask_token_id)[0].detach().cpu().tolist()[0]\n",
    "tokenizer2.convert_ids_to_tokens(torch.topk(logits[0,mask_token_index,:], 10)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4654568",
   "metadata": {},
   "source": [
    "What happens when you push in negative affect direction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eac84c84",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m hook \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mroberta\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m11\u001b[39m]\u001b[38;5;241m.\u001b[39mregister_forward_hook(\u001b[38;5;28;01mlambda\u001b[39;00m m, h_in, h_out: intervention(h_out\u001b[38;5;241m=\u001b[39mh_out, P\u001b[38;5;241m=\u001b[39mP, ws\u001b[38;5;241m=\u001b[39mWs, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      3\u001b[0m input_dict\u001b[38;5;241m=\u001b[39mtokenizer2(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(\u001b[43mmodel2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      5\u001b[0m hook\u001b[38;5;241m.\u001b[39mremove()\n\u001b[1;32m      6\u001b[0m mask_token_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(input_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m==\u001b[39mtokenizer2\u001b[38;5;241m.\u001b[39mmask_token_id)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:1098\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;124;03m    Used to hide legacy arguments that have been deprecated.\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1098\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1112\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    845\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    846\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    847\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    850\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    851\u001b[0m )\n\u001b[0;32m--> 852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    865\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    518\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    520\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1547\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1547\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1550\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(m, h_in, h_out)\u001b[0m\n\u001b[1;32m      1\u001b[0m set_seed(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m hook \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mroberta\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m11\u001b[39m]\u001b[38;5;241m.\u001b[39mregister_forward_hook(\u001b[38;5;28;01mlambda\u001b[39;00m m, h_in, h_out: \u001b[43mintervention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m input_dict\u001b[38;5;241m=\u001b[39mtokenizer2(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(model2(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_dict)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mintervention\u001b[0;34m(h_out, P, ws, alpha)\u001b[0m\n\u001b[1;32m      7\u001b[0m h_tokens \u001b[38;5;241m=\u001b[39m h_out[\u001b[38;5;241m0\u001b[39m][:,\u001b[38;5;241m1\u001b[39m:,:]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# AlterRep code starts here\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m signs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msign(\u001b[43mh_tokens\u001b[49m\u001b[38;5;129;43m@ws\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# h_r component\u001b[39;00m\n\u001b[1;32m     13\u001b[0m proj \u001b[38;5;241m=\u001b[39m (h_tokens\u001b[38;5;129m@ws\u001b[39m\u001b[38;5;241m.\u001b[39mT)\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "set_seed(1)\n",
    "hook = model2.roberta.encoder.layer[11].register_forward_hook(lambda m, h_in, h_out: intervention(h_out=h_out, P=P, ws=Ws, alpha=-4))\n",
    "input_dict=tokenizer2(text, return_tensors='pt')\n",
    "logits = torch.log_softmax(model2(**input_dict)['logits'], dim=2)\n",
    "hook.remove()\n",
    "mask_token_index = torch.where(input_dict['input_ids'][0]==tokenizer2.mask_token_id)[0].detach().cpu().tolist()[0]\n",
    "tokenizer2.convert_ids_to_tokens(torch.topk(logits[0,mask_token_index,:], 10)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12713c5e",
   "metadata": {},
   "source": [
    "What about positive affect direction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "707119c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m hook \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mroberta\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m11\u001b[39m]\u001b[38;5;241m.\u001b[39mregister_forward_hook(\u001b[38;5;28;01mlambda\u001b[39;00m m, h_in, h_out: intervention(h_out\u001b[38;5;241m=\u001b[39mh_out, P\u001b[38;5;241m=\u001b[39mP, ws\u001b[38;5;241m=\u001b[39mWs, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      2\u001b[0m input_dict\u001b[38;5;241m=\u001b[39mtokenizer2(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(\u001b[43mmodel2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m hook\u001b[38;5;241m.\u001b[39mremove()\n\u001b[1;32m      5\u001b[0m mask_token_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(input_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m==\u001b[39mtokenizer2\u001b[38;5;241m.\u001b[39mmask_token_id)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:1098\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;124;03m    Used to hide legacy arguments that have been deprecated.\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1098\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1112\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    845\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    846\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    847\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    850\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    851\u001b[0m )\n\u001b[0;32m--> 852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    865\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    518\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    520\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1547\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1547\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1550\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(m, h_in, h_out)\u001b[0m\n\u001b[1;32m      1\u001b[0m set_seed(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m hook \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mroberta\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m11\u001b[39m]\u001b[38;5;241m.\u001b[39mregister_forward_hook(\u001b[38;5;28;01mlambda\u001b[39;00m m, h_in, h_out: \u001b[43mintervention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m input_dict\u001b[38;5;241m=\u001b[39mtokenizer2(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(model2(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_dict)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mintervention\u001b[0;34m(h_out, P, ws, alpha)\u001b[0m\n\u001b[1;32m      7\u001b[0m h_tokens \u001b[38;5;241m=\u001b[39m h_out[\u001b[38;5;241m0\u001b[39m][:,\u001b[38;5;241m1\u001b[39m:,:]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# AlterRep code starts here\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m signs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msign(\u001b[43mh_tokens\u001b[49m\u001b[38;5;129;43m@ws\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# h_r component\u001b[39;00m\n\u001b[1;32m     13\u001b[0m proj \u001b[38;5;241m=\u001b[39m (h_tokens\u001b[38;5;129m@ws\u001b[39m\u001b[38;5;241m.\u001b[39mT)\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "hook = model2.roberta.encoder.layer[11].register_forward_hook(lambda m, h_in, h_out: intervention(h_out=h_out, P=P, ws=Ws, alpha=4))\n",
    "input_dict=tokenizer2(text, return_tensors='pt')\n",
    "logits = torch.log_softmax(model2(**input_dict)['logits'], dim=2)\n",
    "hook.remove()\n",
    "mask_token_index = torch.where(input_dict['input_ids'][0]==tokenizer2.mask_token_id)[0].detach().cpu().tolist()[0]\n",
    "tokenizer2.convert_ids_to_tokens(torch.topk(logits[0,mask_token_index,:], 10)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad2fb9",
   "metadata": {},
   "source": [
    "What about pushing to be more specific?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dba761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../reps_spec/Ws.layer=11.seed=1.npy\", \"rb\") as f:\n",
    "    Ws = np.load(f)\n",
    "\n",
    "        \n",
    "# Reduce Ws to number of classifiers you want to set it to\n",
    "Ws = Ws[:32:]\n",
    "\n",
    "# Now derive P from Ws\n",
    "list_of_ws = [np.array([Ws[i, :]]) for i in range(Ws.shape[0])]\n",
    "P = debias_by_specific_directions(directions=list_of_ws, input_dim=Ws.shape[1])\n",
    "\n",
    "Ws_sp = torch.tensor(Ws/np.linalg.norm(Ws, keepdims = True, axis = 1)).to(torch.float32)\n",
    "P_sp = torch.tensor(P).to(torch.float32)\n",
    "\n",
    "# Insert newaxis for 1 classifier edge case\n",
    "if len(Ws_sp.shape) == 1:\n",
    "    Ws_sp = Ws_sp[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd4f90d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m hook \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mroberta\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m11\u001b[39m]\u001b[38;5;241m.\u001b[39mregister_forward_hook(\u001b[38;5;28;01mlambda\u001b[39;00m m, h_in, h_out: intervention(h_out\u001b[38;5;241m=\u001b[39mh_out, P\u001b[38;5;241m=\u001b[39mP_sp, ws\u001b[38;5;241m=\u001b[39mWs_sp, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      2\u001b[0m input_dict\u001b[38;5;241m=\u001b[39mtokenizer2(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(\u001b[43mmodel2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m hook\u001b[38;5;241m.\u001b[39mremove()\n\u001b[1;32m      5\u001b[0m mask_token_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(input_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m==\u001b[39mtokenizer2\u001b[38;5;241m.\u001b[39mmask_token_id)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:1098\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;124;03m    Used to hide legacy arguments that have been deprecated.\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1098\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1112\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    845\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    846\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    847\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    850\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    851\u001b[0m )\n\u001b[0;32m--> 852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    865\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    518\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    520\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1547\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1547\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1550\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(m, h_in, h_out)\u001b[0m\n\u001b[1;32m      1\u001b[0m set_seed(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m hook \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mroberta\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m11\u001b[39m]\u001b[38;5;241m.\u001b[39mregister_forward_hook(\u001b[38;5;28;01mlambda\u001b[39;00m m, h_in, h_out: \u001b[43mintervention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m input_dict\u001b[38;5;241m=\u001b[39mtokenizer2(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(model2(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_dict)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mintervention\u001b[0;34m(h_out, P, ws, alpha)\u001b[0m\n\u001b[1;32m      7\u001b[0m h_tokens \u001b[38;5;241m=\u001b[39m h_out[\u001b[38;5;241m0\u001b[39m][:,\u001b[38;5;241m1\u001b[39m:,:]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# AlterRep code starts here\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m signs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msign(\u001b[43mh_tokens\u001b[49m\u001b[38;5;129;43m@ws\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# h_r component\u001b[39;00m\n\u001b[1;32m     13\u001b[0m proj \u001b[38;5;241m=\u001b[39m (h_tokens\u001b[38;5;129m@ws\u001b[39m\u001b[38;5;241m.\u001b[39mT)\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "hook = model2.roberta.encoder.layer[11].register_forward_hook(lambda m, h_in, h_out: intervention(h_out=h_out, P=P_sp, ws=Ws_sp, alpha=-4))\n",
    "input_dict=tokenizer2(text, return_tensors='pt')\n",
    "logits = torch.log_softmax(model2(**input_dict)['logits'], dim=2)\n",
    "hook.remove()\n",
    "mask_token_index = torch.where(input_dict['input_ids'][0]==tokenizer2.mask_token_id)[0].detach().cpu().tolist()[0]\n",
    "tokenizer2.convert_ids_to_tokens(torch.topk(logits[0,mask_token_index,:], 10)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f280ebd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m hook \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mroberta\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m11\u001b[39m]\u001b[38;5;241m.\u001b[39mregister_forward_hook(\u001b[38;5;28;01mlambda\u001b[39;00m m, h_in, h_out: intervention(h_out\u001b[38;5;241m=\u001b[39mh_out, P\u001b[38;5;241m=\u001b[39mP, ws\u001b[38;5;241m=\u001b[39mWs, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      2\u001b[0m input_dict\u001b[38;5;241m=\u001b[39mtokenizer2(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(\u001b[43mmodel2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m hook\u001b[38;5;241m.\u001b[39mremove()\n\u001b[1;32m      5\u001b[0m mask_token_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(input_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m==\u001b[39mtokenizer2\u001b[38;5;241m.\u001b[39mmask_token_id)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:1098\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;124;03m    Used to hide legacy arguments that have been deprecated.\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1098\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1112\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    845\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    846\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    847\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    850\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    851\u001b[0m )\n\u001b[0;32m--> 852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    865\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    518\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    520\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1547\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1547\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1550\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(m, h_in, h_out)\u001b[0m\n\u001b[1;32m      1\u001b[0m set_seed(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m hook \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mroberta\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m11\u001b[39m]\u001b[38;5;241m.\u001b[39mregister_forward_hook(\u001b[38;5;28;01mlambda\u001b[39;00m m, h_in, h_out: \u001b[43mintervention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m input_dict\u001b[38;5;241m=\u001b[39mtokenizer2(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(model2(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_dict)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mintervention\u001b[0;34m(h_out, P, ws, alpha)\u001b[0m\n\u001b[1;32m      7\u001b[0m h_tokens \u001b[38;5;241m=\u001b[39m h_out[\u001b[38;5;241m0\u001b[39m][:,\u001b[38;5;241m1\u001b[39m:,:]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# AlterRep code starts here\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m signs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msign(\u001b[43mh_tokens\u001b[49m\u001b[38;5;129;43m@ws\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# h_r component\u001b[39;00m\n\u001b[1;32m     13\u001b[0m proj \u001b[38;5;241m=\u001b[39m (h_tokens\u001b[38;5;129m@ws\u001b[39m\u001b[38;5;241m.\u001b[39mT)\n",
      "File \u001b[0;32m~/micromamba/envs/nlp/lib/python3.8/site-packages/torch/_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "hook = model2.roberta.encoder.layer[11].register_forward_hook(lambda m, h_in, h_out: intervention(h_out=h_out, P=P, ws=Ws, alpha=4))\n",
    "input_dict=tokenizer2(text, return_tensors='pt')\n",
    "logits = torch.log_softmax(model2(**input_dict)['logits'], dim=2)\n",
    "hook.remove()\n",
    "mask_token_index = torch.where(input_dict['input_ids'][0]==tokenizer2.mask_token_id)[0].detach().cpu().tolist()[0]\n",
    "tokenizer2.convert_ids_to_tokens(torch.topk(logits[0,mask_token_index,:], 10)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b7804",
   "metadata": {},
   "source": [
    "What about the finetuned model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17b47da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sworn',\n",
       " '/status/@@',\n",
       " 'Agg@@',\n",
       " 'LYR@@',\n",
       " 'bib',\n",
       " '#forex@@',\n",
       " '#call@@',\n",
       " 'JAP@@',\n",
       " 'ODY',\n",
       " 'Shim@@']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hook = model.roberta.encoder.layer[11].register_forward_hook(lambda m, h_in, h_out: intervention(h_out=h_out, P=P_aff, ws=Ws_aff, alpha=-4))\n",
    "input_dict=tokenizer(text, return_tensors='pt')\n",
    "logits = torch.log_softmax(model(**input_dict)['logits'], dim=2)\n",
    "hook.remove()\n",
    "mask_token_index = torch.where(input_dict['input_ids'][0]==tokenizer.mask_token_id)[0].detach().cpu().tolist()[0]\n",
    "tokenizer.convert_ids_to_tokens(torch.topk(logits[0,mask_token_index,:], 10)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d934653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ousem@@',\n",
       " 'essm@@',\n",
       " 'Humph@@',\n",
       " 'ask@@',\n",
       " 'Esca@@',\n",
       " 'enth@@',\n",
       " 'Memo@@',\n",
       " '-Le@@',\n",
       " 'enan@@',\n",
       " '#tvtime']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hook = model.roberta.encoder.layer[11].register_forward_hook(lambda m, h_in, h_out: intervention(h_out=h_out, P=P_aff, ws=Ws_aff, alpha=4))\n",
    "input_dict=tokenizer(text, return_tensors='pt')\n",
    "logits = torch.log_softmax(model(**input_dict)['logits'], dim=2)\n",
    "hook.remove()\n",
    "mask_token_index = torch.where(input_dict['input_ids'][0]==tokenizer.mask_token_id)[0].detach().cpu().tolist()[0]\n",
    "tokenizer.convert_ids_to_tokens(torch.topk(logits[0,mask_token_index,:], 10)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bb4b09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['opport@@',\n",
       " 'COMM@@',\n",
       " 'ask@@',\n",
       " '#Fitstats_en_@@',\n",
       " 'mug@@',\n",
       " 'Humph@@',\n",
       " 'Narr@@',\n",
       " 'Fah@@',\n",
       " 'ˈ@@',\n",
       " '\\x93']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hook = model.roberta.encoder.layer[11].register_forward_hook(lambda m, h_in, h_out: intervention(h_out=h_out, P=P_sp, ws=Ws_sp, alpha=-4))\n",
    "input_dict=tokenizer(text, return_tensors='pt')\n",
    "logits = torch.log_softmax(model(**input_dict)['logits'], dim=2)\n",
    "hook.remove()\n",
    "mask_token_index = torch.where(input_dict['input_ids'][0]==tokenizer.mask_token_id)[0].detach().cpu().tolist()[0]\n",
    "tokenizer.convert_ids_to_tokens(torch.topk(logits[0,mask_token_index,:], 10)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f81bb848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chain@@',\n",
       " '#AMNDBots',\n",
       " '#Nar@@',\n",
       " 'Nuke',\n",
       " 'Fail@@',\n",
       " '#iHeartAwards',\n",
       " 'Ends',\n",
       " '#Smule',\n",
       " 'Spoken',\n",
       " '#TheyreTheOne']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hook = model.roberta.encoder.layer[11].register_forward_hook(lambda m, h_in, h_out: intervention(h_out=h_out, P=P_sp, ws=Ws_sp, alpha=4))\n",
    "input_dict=tokenizer(text, return_tensors='pt')\n",
    "logits = torch.log_softmax(model(**input_dict)['logits'], dim=2)\n",
    "hook.remove()\n",
    "mask_token_index = torch.where(input_dict['input_ids'][0]==tokenizer.mask_token_id)[0].detach().cpu().tolist()[0]\n",
    "tokenizer.convert_ids_to_tokens(torch.topk(logits[0,mask_token_index,:], 10)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b4abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
